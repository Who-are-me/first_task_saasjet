{'text': 'Hi, I’m Carrie Anne, and welcome to Crash\nCourse Computer Science!', 'start': 3.2, 'duration': 2.719} {'text': 'Today, let’s start by thinking about how\nimportant vision can be.', 'start': 5.919, 'duration': 3.371} {'text': 'Most people rely on it to prepare food, walk\naround obstacles, read street signs, watch', 'start': 9.29, 'duration': 4.47} {'text': 'videos like this, and do hundreds of other\ntasks.', 'start': 13.76, 'duration': 2.52} {'text': 'Vision is the highest bandwidth sense, and\nit provides a firehose of information about', 'start': 16.28, 'duration': 3.89} {'text': 'the state of the world and how to act on it.', 'start': 20.17, 'duration': 1.94} {'text': 'For this reason, computer scientists have\nbeen trying to give computers vision for half', 'start': 22.11, 'duration': 4.11} {'text': 'a century, birthing the sub-field of computer\nvision.', 'start': 26.22, 'duration': 3.2} {'text': 'Its goal is to give computers the ability\nto extract high-level understanding from digital', 'start': 29.42, 'duration': 4.139} {'text': 'images and videos.', 'start': 33.559, 'duration': 1.48} {'text': 'As everyone with a digital camera or smartphone\nknows, computers are already really good at', 'start': 35.04, 'duration': 4.13} {'text': 'capturing photos with incredible fidelity\nand detail – much better than humans in fact.', 'start': 39.17, 'duration': 4.57} {'text': 'But as computer vision professor Fei-Fei Li\nrecently said, “Just like to hear is the', 'start': 43.82, 'duration': 4.48} {'text': 'not the same as to listen.', 'start': 48.3, 'duration': 1.58} {'text': 'To take pictures is not the same as to see.”', 'start': 49.88, 'duration': 2.56} {'text': 'INTRO', 'start': 52.44, 'duration': 8.88} {'text': 'As a refresher, images on computers are most\noften stored as big grids of pixels.', 'start': 61.32, 'duration': 4.66} {'text': 'Each pixel is defined by a color, stored as\na combination of three additive primary colors:', 'start': 65.98, 'duration': 4.49} {'text': 'red, green and blue.', 'start': 70.47, 'duration': 1.38} {'text': 'By combining different intensities of these\nthree colors, what’s called a RGB value,', 'start': 71.85, 'duration': 2.64} {'text': 'we can represent any color.', 'start': 74.49, 'duration': 3.23} {'text': 'Perhaps the simplest computer vision algorithm\n– and a good place to start – is to track', 'start': 77.72, 'duration': 3.93} {'text': 'a colored object, like a bright pink ball.', 'start': 81.65, 'duration': 2.28} {'text': 'The first thing we need to do is record the\nball’s color.', 'start': 83.93, 'duration': 2.96} {'text': 'For that, we’ll take the RGB value of the\ncentermost pixel.', 'start': 86.89, 'duration': 3.2} {'text': 'With that value saved, we can give a computer\nprogram an image, and ask it to find the pixel', 'start': 90.09, 'duration': 4.32} {'text': 'with the closest color match.', 'start': 94.41, 'duration': 1.57} {'text': 'An algorithm like this might start in the\nupper right corner, and check each pixel,', 'start': 95.98, 'duration': 4.01} {'text': 'one at time, calculating the difference from\nour target color.', 'start': 99.99, 'duration': 3.07} {'text': 'Now, having looked at every pixel, the best\nmatch is very likely a pixel from our ball.', 'start': 103.06, 'duration': 4.44} {'text': 'We’re not limited to running this algorithm\non a single photo; we can do it for every', 'start': 107.5, 'duration': 3.961} {'text': 'frame in a video, allowing us to track the\nball over time.', 'start': 111.461, 'duration': 3.089} {'text': 'Of course, due to variations in lighting,\nshadows, and other effects, the ball on the', 'start': 114.55, 'duration': 3.56} {'text': 'field is almost certainly not going to be\nthe exact same RGB value as our target color,', 'start': 118.11, 'duration': 4.97} {'text': 'but merely the closest match.', 'start': 123.08, 'duration': 1.51} {'text': 'In more extreme cases, like at a game at night,\nthe tracking might be poor.', 'start': 124.59, 'duration': 3.97} {'text': "And if one of the team's jerseys used the\nsame color as the ball, our algorithm would", 'start': 128.56, 'duration': 3.899} {'text': 'get totally confused.', 'start': 132.459, 'duration': 1.441} {'text': 'For these reasons, color marker tracking and\nsimilar algorithms are rarely used, unless', 'start': 133.9, 'duration': 4.61} {'text': 'the environment can be tightly controlled.', 'start': 138.51, 'duration': 2.059} {'text': 'This color tracking example was able to search\npixel-by-pixel, because colors are stored', 'start': 140.569, 'duration': 4.7} {'text': 'inside of single pixels.', 'start': 145.269, 'duration': 1.83} {'text': 'But this approach doesn’t work for features\nlarger than a single pixel, like edges of', 'start': 147.099, 'duration': 3.991} {'text': 'objects, which are inherently made up of many\npixels.', 'start': 151.09, 'duration': 3.149} {'text': 'To identify these types of features in images,\ncomputer vision algorithms have to consider', 'start': 154.239, 'duration': 4.47} {'text': 'small regions of pixels, called patches.', 'start': 158.709, 'duration': 2.221} {'text': 'As an example, let’s talk about an algorithm\nthat finds vertical edges in a scene, let’s', 'start': 160.93, 'duration': 4.44} {'text': 'say to help a drone navigate safely through\na field of obstacles.', 'start': 165.37, 'duration': 2.99} {'text': 'To keep things simple, we’re going to convert\nour image into grayscale, although most algorithms', 'start': 168.36, 'duration': 4.28} {'text': 'can handle color.', 'start': 172.64, 'duration': 1.06} {'text': 'Now let’s zoom into one of these poles to\nsee what an edge looks like up close.', 'start': 173.7, 'duration': 3.55} {'text': 'We can easily see where the left edge of the\npole starts, because there’s a change in', 'start': 177.25, 'duration': 3.81} {'text': 'color that persists across many pixels vertically.', 'start': 181.06, 'duration': 3.039} {'text': 'We can define this behavior more formally\nby creating a rule that says the likelihood', 'start': 184.099, 'duration': 4.151} {'text': 'of a pixel being a vertical edge is the magnitude\nof the difference in color between some pixels', 'start': 188.25, 'duration': 4.87} {'text': 'to its left and some pixels to its right.', 'start': 193.12, 'duration': 2.19} {'text': 'The bigger the color difference between these\ntwo sets of pixels, the more likely the pixel', 'start': 195.31, 'duration': 3.829} {'text': 'is on an edge.', 'start': 199.139, 'duration': 1.11} {'text': 'If the color difference is small, it’s probably\nnot an edge at all.', 'start': 200.249, 'duration': 3.11} {'text': 'The mathematical notation for this operation\nlooks like this – it’s called a kernel', 'start': 203.359, 'duration': 3.811} {'text': 'or filter.', 'start': 207.17, 'duration': 1.07} {'text': 'It contains the values for a pixel-wise multiplication, the sum of which is saved into the center pixel.', 'start': 208.24, 'duration': 5.42} {'text': 'Let’s see how this works for our example\npixel.', 'start': 213.66, 'duration': 2.68} {'text': 'I’ve gone ahead and labeled all of the pixels\nwith their grayscale values.', 'start': 216.34, 'duration': 3.259} {'text': 'Now, we take our kernel, and center it over\nour pixel of interest.', 'start': 219.599, 'duration': 3.86} {'text': 'This specifies what each pixel value underneath\nshould be multiplied by.', 'start': 223.459, 'duration': 3.51} {'text': 'Then, we just add up all those numbers.', 'start': 226.969, 'duration': 2.28} {'text': 'In this example, that gives us 147.', 'start': 229.249, 'duration': 2.75} {'text': 'That becomes our new pixel value.', 'start': 231.999, 'duration': 2.021} {'text': 'This operation, of applying a kernel to a\npatch of pixels, is call a convolution.', 'start': 234.02, 'duration': 4.299} {'text': 'Now let’s apply our kernel to another pixel.', 'start': 238.319, 'duration': 2.39} {'text': 'In this case, the result is 1.', 'start': 240.709, 'duration': 1.78} {'text': 'Just 1.', 'start': 242.489, 'duration': 0.751} {'text': 'In other words, it’s a very small color\ndifference, and not an edge.', 'start': 243.24, 'duration': 3.4} {'text': 'If we apply our kernel to every pixel in the\nphoto, the result looks like this, where the', 'start': 246.64, 'duration': 4.16} {'text': 'highest pixel values are where there are strong\nvertical edges.', 'start': 250.8, 'duration': 2.76} {'text': 'Note that horizontal edges, like those platforms\nin the background, are almost invisible.', 'start': 253.56, 'duration': 5.049} {'text': 'If we wanted to highlight those features,\nwe’d have to use a different kernel – one', 'start': 258.609, 'duration': 3.671} {'text': 'that’s sensitive to horizontal edges.', 'start': 262.28, 'duration': 1.599} {'text': 'Both of these edge enhancing kernels are called\nPrewitt Operators, named after their inventor.', 'start': 263.879, 'duration': 5.431} {'text': 'These are just two examples of a huge variety\nof kernels, able to perform many different', 'start': 269.31, 'duration': 4.09} {'text': 'image transformations.', 'start': 273.4, 'duration': 1.519} {'text': 'For example, here’s a kernel that sharpens\nimages.', 'start': 274.919, 'duration': 2.431} {'text': 'And here’s a kernel that blurs them.', 'start': 277.35, 'duration': 2.17} {'text': 'Kernels can also be used like little image\ncookie cutters that match only certain shapes.', 'start': 279.52, 'duration': 4.01} {'text': 'So, our edge kernels looked for image patches\nwith strong differences from right to left', 'start': 283.53, 'duration': 4.71} {'text': 'or up and down.', 'start': 288.24, 'duration': 1.0} {'text': 'But we could also make kernels that are good\nat finding lines, with edges on both sides.', 'start': 289.24, 'duration': 4.489} {'text': 'And even islands of pixels surrounded by contrasting\ncolors.', 'start': 293.729, 'duration': 3.291} {'text': 'These types of kernels can begin to characterize\nsimple shapes.', 'start': 297.02, 'duration': 3.16} {'text': 'For example, on faces, the bridge of the nose\ntends to be brighter than the sides of the', 'start': 300.18, 'duration': 4.6} {'text': 'nose, resulting in higher values for line-sensitive\nkernels.', 'start': 304.78, 'duration': 3.26} {'text': 'Eyes are also distinctive – a dark circle\nsounded by lighter pixels – a pattern other', 'start': 308.04, 'duration': 4.88} {'text': 'kernels are sensitive to.', 'start': 312.93, 'duration': 1.51} {'text': 'When a computer scans through an image, most\noften by sliding around a search window, it', 'start': 314.45, 'duration': 4.171} {'text': 'can look for combinations of features indicative\nof a human face.', 'start': 318.621, 'duration': 3.418} {'text': 'Although each kernel is a weak face detector\nby itself, combined, they can be quite accurate.', 'start': 322.039, 'duration': 4.921} {'text': 'It’s unlikely that a bunch of face-like\nfeatures will cluster together if they’re', 'start': 326.96, 'duration': 3.47} {'text': 'not a face.', 'start': 330.43, 'duration': 1.109} {'text': 'This was the basis of an early and influential\nalgorithm called Viola-Jones Face Detection.', 'start': 331.539, 'duration': 4.251} {'text': 'Today, the hot new algorithms on the block\nare Convolutional Neural Networks.', 'start': 335.79, 'duration': 4.219} {'text': 'We talked about neural nets last episode,\nif you need a primer.', 'start': 340.009, 'duration': 2.981} {'text': 'In short, an artificial neuron – which is\nthe building block of a neural network – takes', 'start': 342.99, 'duration': 4.37} {'text': 'a series of inputs, and multiplies each by\na specified weight, and then sums those values', 'start': 347.36, 'duration': 4.869} {'text': 'all together.', 'start': 352.229, 'duration': 0.991} {'text': 'This should sound vaguely familiar, because\nit’s a lot like a convolution.', 'start': 353.22, 'duration': 3.14} {'text': 'In fact, if we pass a neuron 2D pixel data,\nrather than a one-dimensional list of inputs,', 'start': 356.36, 'duration': 5.1} {'text': 'it’s exactly like a convolution.', 'start': 361.46, 'duration': 2.19} {'text': 'The input weights are equivalent to kernel\nvalues, but unlike a predefined kernel, neural', 'start': 363.65, 'duration': 5.019} {'text': 'networks can learn their own useful kernels\nthat are able to recognize interesting features', 'start': 368.669, 'duration': 3.871} {'text': 'in images.', 'start': 372.54, 'duration': 0.76} {'text': 'Convolutional Neural Networks use banks of\nthese neurons to process image data, each', 'start': 373.3, 'duration': 4.44} {'text': 'outputting a new image, essentially digested\nby different learned kernels.', 'start': 377.76, 'duration': 4.129} {'text': 'These outputs are then processed by subsequent\nlayers of neurons, allowing for convolutions', 'start': 381.889, 'duration': 4.34} {'text': 'on convolutions on convolutions.', 'start': 386.229, 'duration': 2.18} {'text': 'The very first convolutional layer might find\nthings like edges, as that’s what a single', 'start': 388.409, 'duration': 4.28} {'text': 'convolution can recognize, as we’ve already\ndiscussed.', 'start': 392.689, 'duration': 2.41} {'text': 'The next layer might have neurons that convolve\non those edge features to recognize simple', 'start': 395.099, 'duration': 4.481} {'text': 'shapes, comprised of edges, like corners.', 'start': 399.58, 'duration': 2.48} {'text': 'A layer beyond that might convolve on those\ncorner features, and contain neurons that', 'start': 402.06, 'duration': 4.9} {'text': 'can recognize simple objects, like mouths\nand eyebrows.', 'start': 406.96, 'duration': 2.84} {'text': 'And this keeps going, building up in complexity,\nuntil there’s a layer that does a convolution', 'start': 409.8, 'duration': 4.38} {'text': 'that puts it together: eyes, ears, mouth,\nnose, the whole nine yards, and says “ah', 'start': 414.18, 'duration': 4.47} {'text': 'ha, it’s a face!”', 'start': 418.65, 'duration': 1.19} {'text': 'Convolutional neural networks aren’t required\nto be many layers deep, but they usually are,', 'start': 419.84, 'duration': 4.7} {'text': 'in order to recognize complex objects and\nscenes.', 'start': 424.55, 'duration': 2.47} {'text': 'That’s why the technique is considered deep\nlearning.', 'start': 427.02, 'duration': 2.84} {'text': 'Both Viola-Jones and Convolutional Neural\nNetworks can be applied to many image recognition', 'start': 429.86, 'duration': 4.32} {'text': 'problems, beyond faces, like recognizing handwritten\ntext, spotting tumors in CT scans and monitoring', 'start': 434.18, 'duration': 5.049} {'text': 'traffic flow on roads.', 'start': 439.229, 'duration': 1.291} {'text': 'But we’re going to stick with faces.', 'start': 440.52, 'duration': 2.04} {'text': 'Regardless of what algorithm was used, once\nwe’ve isolated a face in a photo, we can', 'start': 442.56, 'duration': 4.3} {'text': 'apply more specialized computer vision algorithms\nto pinpoint facial landmarks, like the tip', 'start': 446.86, 'duration': 5.059} {'text': 'of the nose and corners of the mouth.', 'start': 451.919, 'duration': 2.051} {'text': 'This data can be used for determining things\nlike if the eyes are open, which is pretty', 'start': 453.97, 'duration': 4.06} {'text': 'easy once you have the landmarks – it’s\njust the distance between points.', 'start': 458.03, 'duration': 3.78} {'text': 'We can also track the position of the eyebrows;\ntheir relative position to the eyes can be', 'start': 461.81, 'duration': 4.039} {'text': 'an indicator of surprise, or delight.', 'start': 465.849, 'duration': 2.1} {'text': 'Smiles are also pretty straightforward to\ndetect based on the shape of mouth landmarks.', 'start': 467.949, 'duration': 4.84} {'text': 'All of this information can be interpreted\nby emotion recognition algorithms, giving', 'start': 472.789, 'duration': 4.421} {'text': 'computers the ability to infer when you’re\nhappy, sad, frustrated, confused and so on.', 'start': 477.21, 'duration': 5.07} {'text': 'In turn, that could allow computers to intelligently\nadapt their behavior... maybe offer tips when', 'start': 482.28, 'duration': 5.21} {'text': 'you’re confused, and not ask to install\nupdates when you’re frustrated.', 'start': 487.49, 'duration': 3.62} {'text': 'This is just one example of how vision can\ngive computers the ability to be context sensitive,', 'start': 491.11, 'duration': 5.059} {'text': 'that is, aware of their surroundings.', 'start': 496.169, 'duration': 2.011} {'text': "And not just the physical surroundings – like\nif you're at work or on a train – but also", 'start': 498.18, 'duration': 3.7} {'text': 'your social surroundings – like if you’re\nin a formal business meeting versus a friend’s', 'start': 501.88, 'duration': 4.36} {'text': 'birthday party.', 'start': 506.24, 'duration': 1.06} {'text': 'You behave differently in those surroundings, and so should computing devices, if they’re smart.', 'start': 507.42, 'duration': 4.58} {'text': 'Facial landmarks also capture the geometry\nof your face, like the distance between your', 'start': 512.0, 'duration': 4.18} {'text': 'eyes and the height of your forehead.', 'start': 516.18, 'duration': 1.95} {'text': 'This is one form of biometric data, and it\nallows computers with cameras to recognize', 'start': 518.13, 'duration': 4.4} {'text': 'you.', 'start': 522.53, 'duration': 0.67} {'text': 'Whether it’s your smartphone automatically\nunlocking itself when it sees you, or governments', 'start': 523.2, 'duration': 4.02} {'text': 'tracking people using CCTV cameras, the applications\nof face recognition seem limitless.', 'start': 527.22, 'duration': 5.29} {'text': 'There have also been recent breakthroughs\nin landmark tracking for hands and whole bodies,', 'start': 532.51, 'duration': 4.18} {'text': 'giving computers the ability to interpret\na user’s body language, and what hand gestures', 'start': 536.69, 'duration': 4.149} {'text': 'they’re frantically waving at their internet\nconnected microwave.', 'start': 540.839, 'duration': 2.731} {'text': 'As we’ve talked about many times in this\nseries, abstraction is the key to building', 'start': 543.57, 'duration': 3.98} {'text': 'complex systems, and the same is true in computer\nvision.', 'start': 547.55, 'duration': 3.33} {'text': 'At the hardware level, you have engineers\nbuilding better and better cameras, giving', 'start': 550.88, 'duration': 3.73} {'text': 'computers improved sight with each passing\nyear, which I can’t say for myself.', 'start': 554.61, 'duration': 4.3} {'text': 'Using that camera data, you have computer\nvision algorithms crunching pixels to find', 'start': 558.91, 'duration': 4.429} {'text': 'things like faces and hands.', 'start': 563.339, 'duration': 1.81} {'text': 'And then, using output from those algorithms,\nyou have even more specialized algorithms', 'start': 565.149, 'duration': 3.971} {'text': 'for interpreting things like user facial expression\nand hand gestures.', 'start': 569.12, 'duration': 3.81} {'text': 'On top of that, there are people building\nnovel interactive experiences, like smart', 'start': 572.93, 'duration': 4.11} {'text': 'TVs and intelligent tutoring systems, that\nrespond to hand gestures and emotion.', 'start': 577.04, 'duration': 4.62} {'text': 'Each of these levels are active areas of research,\nwith breakthroughs happening every year.', 'start': 581.66, 'duration': 4.42} {'text': 'And that’s just the tip of the iceberg.', 'start': 586.08, 'duration': 1.71} {'text': 'Today, computer vision is everywhere – whether\nit’s barcodes being scanned at stores, self-driving', 'start': 587.79, 'duration': 5.14} {'text': 'cars waiting at red lights, or snapchat filters\nsuperimposing mustaches.', 'start': 592.93, 'duration': 3.89} {'text': 'And, the most exciting thing is that computer\nscientists are really just getting started,', 'start': 596.82, 'duration': 4.62} {'text': 'enabled by recent advances in computing, like\nsuper fast GPUs.', 'start': 601.44, 'duration': 4.3} {'text': 'Computers with human-like ability to see is\ngoing to totally change how we interact with them.', 'start': 605.74, 'duration': 5.18} {'text': 'Of course, it’d also be nice if they could\nhear and speak, which we’ll discuss next', 'start': 610.92, 'duration': 3.5} {'text': 'week.', 'start': 614.42, 'duration': 0.58} {'text': 'I’ll see you then.', 'start': 615.04, 'duration': 0.5}